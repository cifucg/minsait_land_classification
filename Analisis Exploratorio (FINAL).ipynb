{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"#004D7F\" size=4>UniversityHack2020</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=5>Reto Minsait Land Classification</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3><a href=\"https://www.linkedin.com/in/cifucg\">Cristian Cifuentes García</a>, Manuel Bermúdez Martínez</font><br>\n",
    "<font color=\"#004D7F\" size=3>Curso de Especialista en Ciencia de Datos y Desarrollo de Aplicaciones en la Nube </font><br>\n",
    "<font color=\"#004D7F\" size=3>Universidad de Castilla-La Mancha</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Requisitos](#section2)\n",
    "* [3. Análisis exploratorio de los datos](#section3)\n",
    "    * [3.1 Tratamiento de las columnas numéricas](#section31)\n",
    "        * [3.1.1 Variables relativas al Geoposicionamiento](#section311)\n",
    "        * [3.1.2 Variables relativas a los Colores](#section312)\n",
    "        * [3.1.3 Variables relativas a la Geometría](#section313)\n",
    "        * [3.1.4 Variables relativas al año de construcción y máximo de pisos de los edificios colindantes](#section314)\n",
    "    * [3.2 Tratamiento de las columnas discretas](#section32)\n",
    "* [4. Preprocesamiento del conjunto de datos](#section4)\n",
    "* [5. Construcción de modelos](#section5)\n",
    "    * [5.1 Construcción de modelo binario](#section51)\n",
    "    * [5.2 Construcción de modelo multietiqueta](#section52)\n",
    "<br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## <font color=\"#004D7F\"> 1. Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Utiliza la información de las imágenes de satélite para clasificar el suelo.</center></h3>\n",
    "\n",
    "Actualmente, un gran número de satélites toman imágenes con distintos fines y usos. El gran número de imágenes y la gran cantidad de datos que se obtienen de las mismas hace necesario crear modelos predictivos para identificar el contenido de la imagen.\n",
    "\n",
    "En este reto ya dispondrás de las variables extraídas de la imagen y georeferenciadas, así como variables categóricas asociadas al entorno para estimar un modelo.\n",
    "\n",
    "<h3><center>Objetivo</center></h3>\n",
    "\n",
    "Te retamos a que encuentres el mejor modelo de clasificación automática de suelos en base a las imágenes proporcionadas por el satélite Sentinel II del servicio Copernicus de la Agencia Espacial Europea.\n",
    "\n",
    "En este reto dispondrás de un conjunto de fincas catastrales asociados a una lista de atributos extraídos de la imagen.\n",
    "\n",
    "Para ello puedes utilizar las distintas técnicas de Machine Learning disponibles para este tipo de problemas.\n",
    "\n",
    "La métrica objetivo a maximizar es la “Exactitud”, (en R, en Python) definida como el “Número de registros correctamente clasificados / Número total de registros proporcionados por la Organización”.\n",
    "\n",
    "<img src=\"data/mapa_introduccion.jpg\">\n",
    "\n",
    "<h3><center>El Dataset</center></h3>\n",
    "\n",
    "El dataset contiene un listado de superficies sobre las que se han recortado la imagen de satélite y se han extraído una serie de características de sus geometrías. Finalmente se ha etiquetado el conjunto de los datos según una clasificación de suelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## <font color=\"#004D7F\"> 2. Requisitos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta celda está destinada a la importación de los paquetes y librerías necesarias para el desarrollo del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i>\n",
    "Es necesario tener instaladas todas las librerías, la mayoría de ellas se pueden instalar mediante el comando `pip` en el terminal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias para el tratamiento de los datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerias para la graficación de los datost\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;\n",
    "sns.set()\n",
    "\n",
    "# Permite que las graficas se generen a mayor resolucion\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# Permite ignorar los warnings de la libreta al generar algunos modelos\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Establece un ancho de libreta mayor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "\n",
    "# Libreria necesaria para los gráficos interactivos\n",
    "from ipywidgets import interact \n",
    "\n",
    "# Librerias necesarias para el aprendizaje de modelos\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Modelos\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#Métricas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función nos va a permitir obtener y visualizar la matriz de confusión de los modelos que entrenemos en la sección [5. Construcción de modelos](#section5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(y, y_pred):\n",
    "    sns.heatmap(confusion_matrix(y, y_pred), square=True, annot=True, fmt='d', cbar=True, cmap=plt.cm.Blues)\n",
    "    plt.ylabel('Clase real')\n",
    "    plt.xlabel('Predicción');\n",
    "    plt.gca().set_ylim(2.0, 0)\n",
    "    plt.show()\n",
    "    print(\"Resultados\")\n",
    "    print('Accuracy: {}'.format(round(accuracy_score(y, y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el conjunto de datos proporcionado en un fichero de texto plano y lo convertimos en un *dataframe* de *pandas*. Este operación tiene la finalidad de permitirnos trabajar de una forma más rápida y eficiente con el conjunto de datos y así, poder realizar un buen análisis exploratorio del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar = pd.read_csv('data/Modelar_UH2020.txt', sep=\"|\", index_col='ID', encoding='utf-8')\n",
    "print(\"Tamaño del conjunto de datos:  %d\" % df_modelar.shape[0])\n",
    "print(\"Número de variables: %d\" % df_modelar.shape[1])\n",
    "if df_modelar.index.is_unique:\n",
    "    print('El índice es único.')\n",
    "else:\n",
    "    print('Los índices están duplicados.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que contamos con un total de <b>103230 datos</b>, es decir, con una gran cantidad de registros. Este elevado número nos puede ayudar, en gran parte, a desarrollar un modelo con un rendimiento acertado. Por otro lado, contamos con <b>56 variables</b>, pero se ha establecido como índice del conjunto de datos la variable *ID*, tras comprobar que realmente es un identificador único y no se repite, por lo que, <b>el número de variables disminuye a 55.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_class_order = ['RESIDENTIAL', 'PUBLIC', 'RETAIL', 'OFFICE', 'INDUSTRIAL', 'AGRICULTURE', 'OTHER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta lista presenta como finalidad la representación de las variables en el mismo orden y con el mismo color asociado en todas aquellas gráficas que las utilicemos. Y en la siguiente celda se define una lista en la cual se irán añadiendo las diferentes funciones que se aplicarán en el apartado de preprocesamiento del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_preprocess_function = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section3\"></a>\n",
    "## <font color=\"#004D7F\"> 3. Análisis exploratorio de los datos</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una primera toma de contacto con los datos, podemos ver una pequeña parte del conjunto para ver qué valores toman cada una de las 55 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables y el tipo de datos de cada una de ellas es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_modelar.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve a simple vista que predominan las variables numéricas y que el conjunto de datos presenta muy pocas variables discretas. De todas formas, realizaremos un tratamiento específico para cada una de ellas en el apartado correspondiente. \n",
    "\n",
    "Hay que tener en cuenta que el principal problema de este reto es el desbalanceo de las clases y por ello mismo, se debe de comprobar cuantos registros tenemos para cada uno de los diferentes valores de la variables a objetivo o a predecir, que es la variable `Clase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(x='CLASE', data=df_modelar)\n",
    "plt.title('Distribución de muestras')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gráfica se puede observar que nos encontramos ante un conjunto de datos muy desbalanceado, ya que el mayor porcentaje de datos corresponde a la clase <b>*Residential*</b> y existe una gran diferencia con el resto de clases del conjunto de datos. De hecho, si nos fijamos a continuación podemos ver el porcentaje que representa cada una de las clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_modelar['CLASE'].unique().tolist():\n",
    "    reg = sum(df_modelar['CLASE']==item)\n",
    "    print(f\"- \\033[1m{item}\\033[0m presenta un \\033[1m{(reg / df_modelar.shape[0]):.3f}\\033[0m% del total con: \\033[1m{reg}\\033[0m registros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partiendo de estos datos, es necesario realizar un profundo análisis de todas columnas de nuestro conjunto de datos, ya que el número total de registros es muy elevado, así como el número de características que presenta el dataset. Para ello, nos centraremos en dividir estas características en función del tipo de dato que representen, como hemos podido observar anteriormente.\n",
    "\n",
    "En la siguiente celda realizamos esta división, es decir, dividimos las variables en dos listas dependiendo del tipo de dato que representen. Además, en las variables discretas eliminamos la variable `Clase` ya que se trata de la variable objetivo y por el momento no se va a realizar ningún preprocesamiento en ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_df_columns = df_modelar.select_dtypes(exclude=np.number).columns.tolist()\n",
    "dis_df_columns.remove('CLASE') #Eliminamos la variable clase\n",
    "number_df_columns = df_modelar.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "print('- \\033[1mVariables discretas\\033[0m: ',dis_df_columns)\n",
    "print('\\n- \\033[1mVariables numéricas\\033[0m: ',number_df_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos hecha la división de forma correcta, debemos de realizar un el tratamiento de forma independiente y así, comprobar si realmente pertenecen al tipo de datos que representa la variable o se pueden convertir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a>\n",
    "### <font color=\"#004D7F\">3.1 Tratamiento de las columnas numéricas </font>\n",
    "\n",
    "El tratamiento de las columnas numéricas es relativamente sencillo, y se puede descomponer en varias etapas:\n",
    "\n",
    "* Comprobar que, efectivamente, corresponden a características numéricas. \n",
    "* Detección y tratamiento de outliers. \n",
    "* Detección y tratamiento de valores perdidos. \n",
    "* Exploración de las variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que se comprueba es su realmente estas variables son numéricas o se pueden convertir a discretas, por ejemplo, si una variable tiene unicamente tres valores se convertirá en una variable discreta en vez de numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(col, len(df_modelar[col].value_counts())) for col in number_df_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es curioso ver que en las variables que representan los colores se repiten bastante los valores obteniendo 230 valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que parece, todas las variables ellas son numéricas y no se debe de realizar ningun cambio para ellas. Al tener un alto número de variables vamos a realizar subdivisiones en función de la identificación de cada una de ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Geoposición**: 'X', 'Y'\n",
    "- **Colores:** 'Q_R_4_0_0', 'Q_R_4_0_1', 'Q_R_4_0_2', 'Q_R_4_0_3', 'Q_R_4_0_4', 'Q_R_4_0_5', 'Q_R_4_0_6', 'Q_R_4_0_7', 'Q_R_4_0_8', 'Q_R_4_0_9', 'Q_R_4_1_0', 'Q_G_3_0_0', 'Q_G_3_0_1', 'Q_G_3_0_2', 'Q_G_3_0_3', 'Q_G_3_0_4', 'Q_G_3_0_5', 'Q_G_3_0_6', 'Q_G_3_0_7', 'Q_G_3_0_8', 'Q_G_3_0_9', 'Q_G_3_1_0', 'Q_B_2_0_0', 'Q_B_2_0_1', 'Q_B_2_0_2', 'Q_B_2_0_3', 'Q_B_2_0_4', 'Q_B_2_0_5', 'Q_B_2_0_6', 'Q_B_2_0_7', 'Q_B_2_0_8', 'Q_B_2_0_9', 'Q_B_2_1_0', 'Q_NIR_8_0_0', 'Q_NIR_8_0_1', 'Q_NIR_8_0_2', 'Q_NIR_8_0_3', 'Q_NIR_8_0_4', 'Q_NIR_8_0_5', 'Q_NIR_8_0_6', 'Q_NIR_8_0_7', 'Q_NIR_8_0_8', 'Q_NIR_8_0_9', 'Q_NIR_8_1_0'\n",
    "- **Geométricas:** 'AREA', 'GEOM_R1', 'GEOM_R2', 'GEOM_R3', 'GEOM_R4'\n",
    "- **Otras:** 'CONTRUCTIONYEAR', 'MAXBUILDINGFLOOR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoposition = ['X', 'Y']\n",
    "colors = ['Q_R_4_0_0', 'Q_R_4_0_1', 'Q_R_4_0_2', 'Q_R_4_0_3', 'Q_R_4_0_4', 'Q_R_4_0_5', 'Q_R_4_0_6', 'Q_R_4_0_7',\n",
    "          'Q_R_4_0_8', 'Q_R_4_0_9', 'Q_R_4_1_0', 'Q_G_3_0_0', 'Q_G_3_0_1', 'Q_G_3_0_2', 'Q_G_3_0_3', 'Q_G_3_0_4',\n",
    "          'Q_G_3_0_5', 'Q_G_3_0_6', 'Q_G_3_0_7', 'Q_G_3_0_8', 'Q_G_3_0_9', 'Q_G_3_1_0', 'Q_B_2_0_0', 'Q_B_2_0_1',\n",
    "          'Q_B_2_0_2', 'Q_B_2_0_3', 'Q_B_2_0_4', 'Q_B_2_0_5', 'Q_B_2_0_6', 'Q_B_2_0_7', 'Q_B_2_0_8', 'Q_B_2_0_9',\n",
    "          'Q_B_2_1_0', 'Q_NIR_8_0_0', 'Q_NIR_8_0_1', 'Q_NIR_8_0_2', 'Q_NIR_8_0_3', 'Q_NIR_8_0_4', 'Q_NIR_8_0_5', \n",
    "          'Q_NIR_8_0_6', 'Q_NIR_8_0_7', 'Q_NIR_8_0_8', 'Q_NIR_8_0_9', 'Q_NIR_8_1_0']\n",
    "geom = ['AREA', 'GEOM_R1', 'GEOM_R2', 'GEOM_R3', 'GEOM_R4']\n",
    "others = ['CONTRUCTIONYEAR', 'MAXBUILDINGFLOOR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section311\"></a>\n",
    "#### <font color=\"#004D7F\">3.1.1 Variables relativas al Geoposicionamiento</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información de longitud-latitud relativa a las variables **X** e **Y** ha sido escalada y desplazada aleatoriamente (manteniendo la relación con el resto de registros). Aún así vamos a intentar dibujar las geolocalizaciones que tenemos en el conjunto de datos con respecto a la latitud y la longitud para hacernos una idea de las parcelas y de la clase que presentan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente comprobamos si presenta o no valores perdidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar[geoposition].isna().sum()[df_modelar[geoposition].isna().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no presenta valores perdidos y por lo tanto no es realizar ningun tipo preprocesamiento a estas variables.\n",
    " \n",
    "A continuación vamos a mostrar todas las geoposiciones de todo el conjunto de datos para hacernos una idea de la localización, la dispersión de los valores y ver si podemos discriminar por zonas, o incluso agrupar para poder hacer una primera aproximación de modelo supervisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "sns.scatterplot(x=\"X\", y=\"Y\", hue=\"CLASE\",data=df_modelar)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL // Yo dejaria ambas\n",
    "import plotly.express as px\n",
    "fig = px.scatter(df_modelar, x=\"X\", y=\"Y\", color=\"CLASE\", hover_data=['CLASE'])\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos previamente al predominar registros cuya clase es `Residential`, se ve reflejado perfectamente en el mapa proyectado, situandose la mayor parte de registros en la parte central, mientras que clases como `Agriculture` o `Industrial` se sitúan en zonas de extrarradio de la ciudad de Madrid.\n",
    "Sin embargo, el resto de clases se encuentran mezcladas, o es más complejo discriminar con la clase predominante, lo cual presenta sentido, ya que las tiendas o los lugares públicos no se suelen situar en zonas de extrarradio, salvo en casos excepcionales.\n",
    "\n",
    "Así mismo, vamos a analizar cada una de las clases con respecto a la geolocalización de los registros, con el objetivo de abstraernos del resto de clases y que no nos afecte la clase predominante a la hora de graficar para ver que información nos pueden trasmitir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"X\", y=\"Y\", col=\"CLASE\", hue=\"CLASE\", kind=\"scatter\", col_wrap=3, data=df_modelar);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos comentado previamente, podemos observar que los registros pertenecientes a la clase de `Agricultura` se encuentran en zonas de extrarradio, mientras que el resto de registros están mas céntricos. Por otro lado se pueden ver pequeñas zonas o agrupaciones de registros, que probablemente con algun tipo de modelo de clasificación mediante *clusters* se podrían clasificar correctamente u obtener un buen rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section312\"></a>\n",
    "#### <font color=\"#004D7F\">3.1.2 Variables relativas a los Colores</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables referentes a los colores se corresponden a información obtenida de laa imágenes proporcionadas por el satélite Sentinel II del servicio Copernicus de la Agencia Espacial Europea.\n",
    "Las imágenes satelitales se han tratado y se ha extraído información de 4 canales (R, G, B y NIR), correspondientes a las bandas de color rojo, verde y azul, y el infrarrojo cercano.\n",
    "El valor mostrado en cada una de estas variables corresponde a la intensidad por deciles en cada imagen.\n",
    "\n",
    "La banda de color `rojo` corresponde a la banda 4 del espectro visible, el `verde` a la banda 3 y el `azul` a la banda 2, mientras que el `infrarrojo cercano` hace referencia a la banda 8 del espectro electromagnético. Dichas bandas junto con el valor de su longitud de onda central se puede observar en la siguiente imagen. Además, todas ellas presentan una resolución de **10m/px**.\n",
    "\n",
    "Con las bandas 8-4-3 podemos ver la vegetación en tonos rojos, las zonas urbanas son de color azul cian, y los suelos varían de marrón oscuro (también zonas quemadas) a marrón claro. El hielo, la nieve y las nubes son blancos o cian claro. Esta es una combinación de banda muy popular y es útil para estudios de vegetación, monitoreo de drenaje y patrones de suelo y varias etapas de crecimiento de cultivos. En general, los tonos rojos intensos indican hojas anchas y/o vegetación más sana, mientras que los rojos más claros significan pastizales o áreas escasamente vegetadas. Las áreas urbanas densamente pobladas se muestran en azul claro. Esta combinación de bandas ofrece resultados similares a la fotografía aérea infrarroja tradicional.\n",
    "\n",
    "Para más información con respecto al Satelite Sentinel II y sus propiedades: https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/sentinel_resolution.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con respecto a las variables que presenta el conjunto de datos, todas empiezan con la letra **Q** seguidas del color siendo R, G, B o NIR, seguidamente el número de la banda que corresponde a cada uno de los colores siendo estos: 4, 3, 2 y 8, y finalmente, disponemos de 11 registros por cada una de las bandas, ya que tenemos valores desde el 0 al 10.\n",
    "\n",
    "Por ejemplo de la banda roja tenemos del Q_R_4_0_0 al Q_R_4_1_0, teniendo entre medias todas las variables referentes a la banda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente comprobamos si presenta o no valores perdidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar[colors].isna().sum()[df_modelar[colors].isna().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no presenta valores perdidos y por lo tanto no es realizar ningun tipo de análisis extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_modelar[colors].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estudiar la correlación de todas y cada una de las variables relacionadas con el color es inviable y no se observarían correctamente los datos, lo que haremos es un agrupamiento por colores, es decir, tendremos lo siguiente:\n",
    "\n",
    "- **Rojo**\n",
    "- **Verde**\n",
    "- **Azul**\n",
    "- **NIR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlación perteneciente al color rojo\n",
    "df_modelar[colors[:11]].corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el color rojo podemos observar que existe una gran correlación entre casi todas las variables, excepto con las pertenecientes al decil 0 y 10. Por otro lado se puede observar que cada una de ellas presenta una mayor correlación con su respectivo antecesor y sucesor. Por ejemplo, la variable *Q_R_4_0_5* presenta una mayor correlación con *Q_R_4_0_4* y *Q_R_4_0_6*\n",
    "\n",
    "Como excepción, decir que el decil 9 (*Q_R_4_0_9*) presenta una mayor correlación con sus dos antecesores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlación perteneciente al color verde\n",
    "df_modelar[colors[11:22]].corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del color verde ocurre exactamente lo mismo que para el color rojo, ya que la máxima correlación la presentan con su decil anterior y posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlación perteneciente al color azul\n",
    "df_modelar[colors[22:33]].corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocurre lo mismo con el color azul y también para el NIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlación perteneciente al color azul\n",
    "df_modelar[colors[33:44]].corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de ver y comprobar la correlación que presentan este tipo de variables, también se ha realizado un pequeño análisis para comprender qué representan realmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_colors = [df_modelar.columns[2:13], df_modelar.columns[13:24], df_modelar.columns[24:35], df_modelar.columns[35:46]]\n",
    "colors = [ 'red', 'green', 'blue', 'gray']\n",
    "for idx, val in enumerate(list_colors):\n",
    "    plt.figure(idx, figsize=(16,2))\n",
    "    sns.barplot(x=df_modelar.iloc[1][val].index, y=df_modelar.iloc[1][val].values, color=colors[idx], label=colors[idx])\n",
    "    plt.legend()\n",
    "    plt.plot()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = df_modelar[df_modelar.columns[2:-42]]\n",
    "df_green = df_modelar[df_modelar.columns[13:-31]]\n",
    "df_blue = df_modelar[df_modelar.columns[24:-20]]\n",
    "df_nir = df_modelar[df_modelar.columns[35:-9]]\n",
    "\n",
    "df_sum = df_red.sum(axis=1) + df_green.sum(axis=1) + df_blue.sum(axis=1) + df_nir.sum(axis=1)\n",
    "df_sum.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta gráfica podemos ver que los valores van creciendo progresivamente en función del decil en el cual se encuentran, por lo que descartamos que se trate de un histograma. Además, en el caso de ser un histograma, el valor total de la suma de las diferentes variables correspondientes a los colores debería de ser igual para cada uno de los distintos registros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section313\"></a>\n",
    "#### <font color=\"#004D7F\">3.1.3 Variables relativas a la Geometría</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con respecto a estas variables no tenemos demasiada información, lo único que sabemos es que las métricas geométricas se encuentran generadas automáticamente, bajo el prefijo **GEOM** y la variable **AREA** que corresponde a los metros cuadrados de la parcela a clasificar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente comprobamos si presenta o no valores perdidos. En este caso no presenta valores perdidos y por lo tanto no es realizar ningun tipo de análisis extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_modelar[geom].isna().sum()[df_modelar[geom].isna().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_modelar[geom].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso podemos observar que existe una gran diferencia entre los valores máximos y mínimos, por lo que vamos a mostrar diagrama de cajas para observar esta varianza y comprobar si es correcta.\n",
    "En el siguiente diagrama de cajas observamos que los datos se encuentran demasiado alejados de la media y la mediana, por lo tanto, creemos que podrían ser **outliers**, y sería conveniente realizar un estudio más en profundidad para que en el caso de que sean outliers, decidir si se eliminan o no, ya que pueden aportar ruido al modelo cuando éste sea entrenado con el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for col, ax in enumerate(axs.flatten()):\n",
    "    col_name = geom[col]\n",
    "    sns.boxplot(x=df_modelar[col_name], orient='vertical', ax=ax)\n",
    "    ax.set_title(col_name);    \n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a sacar los 100 elementos cuya area sea mayor que el resto para ver de que tipo de suelo son y por lo tanto poder concluir si se trata de outliers o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar.loc[df_modelar['AREA'].nlargest(100).index.tolist()]['CLASE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que una de las clases minoritarias es **Residential**, por lo tanto, al ser la clase predominante en el problema hay que analizar estos valores con respecto a cada clase, no sólo sobre la variabe `AREA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_modelar.groupby('CLASE')['AREA'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.barplot(x=df_modelar.groupby('CLASE')['AREA'].mean().index.values, y=df_modelar.groupby('CLASE')['AREA'].mean().values, order=list_class_order)\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Media del área')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este gráfico se puede observar como la variable `RESIDENTIAL` que es la mayoritaría en nuestro conjunto de datos, presenta un área media menor en comparación con el resto de clases a clasificar, siendo esta **281** aproximadamente. Esto nos puede dar una idea del tamaño de cada uno de los terrenos clasificados como `RESIDENTIAL` y se puede comprobar que representan un menor tamaño que el resto.\n",
    "\n",
    "Esto provoca que al haber mayor número de registros con esta media, el resto de valores del resto de clases se muestren como outliers en el diagrama de cajas previamente visualizado, cuando no tiene porqué serlo.\n",
    "\n",
    "De todas formas vamos a realizar un estudio en mayor profundidad eliminando aquellos registros que \"*suponemos*\" que son outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_outliers_geom(df, columns):\n",
    "    df_aux = df.copy()\n",
    "    for col_name in columns:\n",
    "        if col_name == 'AREA':\n",
    "            third_quantile_area = df_aux[col_name].quantile(0.8)\n",
    "            df_aux = df_aux[df_aux[col_name] < third_quantile_area]\n",
    "        elif col_name == 'GEOM_R1':\n",
    "            first_quantile_area = df_aux[col_name].quantile(0.016)\n",
    "            third_quantile_area = df_aux[col_name].quantile(0.92)\n",
    "            df_aux = df_aux[(df_aux[col_name] > first_quantile_area) & (df_aux[col_name] < third_quantile_area)]\n",
    "        elif col_name == 'GEOM_R2':\n",
    "            third_quantile_area = df_aux[col_name].quantile(0.96)\n",
    "            df_aux = df_aux[df_aux[col_name] < third_quantile_area]\n",
    "        elif col_name == 'GEOM_R3':\n",
    "            third_quantile_area = df_aux[col_name].quantile(0.96)\n",
    "            df_aux = df_aux[df_aux[col_name] < third_quantile_area]\n",
    "        elif col_name == 'GEOM_R4':\n",
    "            third_quantile_area = df_aux[col_name].quantile(0.93)\n",
    "            first_quantile_area = df_aux[col_name].quantile(0.005)\n",
    "            df_aux = df_aux[(df_aux[col_name] > first_quantile_area) & (df_aux[col_name] < third_quantile_area)]\n",
    "    return df_aux\n",
    "df_aux = delete_outliers_geom(df_modelar, geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de aplicar una eliminación de los supuestos outliers obtenemos los siguientes diagramas de cajas sin outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for col, ax in enumerate(axs.flatten()):\n",
    "    col_name = geom[col]\n",
    "    sns.boxplot(x=df_aux[col_name], orient='vertical', ax=ax)\n",
    "    ax.set_title(col_name);    \n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset con outliers: {df_modelar.shape}\")\n",
    "print(f\"Dataset sin outliers: {df_aux.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,4))\n",
    "sns.countplot(x='CLASE', data=df_modelar, order=list_class_order, ax=ax1)\n",
    "ax1.set_title('Distribución de muestras con outliers')\n",
    "sns.countplot(x='CLASE', data=df_aux, order=list_class_order, ax=ax2);\n",
    "ax2.set_title('Distribución de muestras sin outliers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La eliminación de outliers reduce considerablemente el número de registros de nuestro conjunto de datos y es por ello por lo que debemos de valorar si se deben de eliminar o no estos outliers, o si realmente se trata de outliers o no. \n",
    "\n",
    "Consideramos que al haber reducido el conjunto de datos original en casi un 50% y en haber perjudicado las clases minoritarias, vamos seguir trabajando con el conjunto de datos original sin eliminar los supuestos outliers, y aceptamos que se tratan de valores normales. Preferimos mantener los registros referentes a las clases minoritarias, a reducir el conjunto de datos eliminando estos registros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables geométricas parecen tener una gran importancia en nuestro conjunto de datos, por lo que es conveniente seguir realizando un análisis exahustivo de ellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_modelar, x=\"CONTRUCTIONYEAR\", y=\"AREA\", animation_frame=\"CONTRUCTIONYEAR\", animation_group=\"CLASE\",\n",
    "           color=\"CLASE\", hover_name=\"CLASE\", facet_col=\"CLASE\",\n",
    "           log_x=True, size_max=45, range_x=[100,100000])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar.groupby('CONTRUCTIONYEAR')[['AREA']].mean().nlargest(10, 'AREA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.lineplot(data=df_modelar.groupby('CONTRUCTIONYEAR')[['AREA']].mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta gráfica se aprecia un pequeño aumento de la media del área conforme avanzan los años y llegamos a la actualidad o a los datos más reciente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section314\"></a>\n",
    "#### <font color=\"#004D7F\">Variables relativas al año de construcción y máximo de pisos de los edificios colindantes</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente comprobamos si presentan o no valores perdidos. En este caso si presenta valores perdidos y por lo tanto es conveniente realizar un análisis extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_modelar[others].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora realizamos una comprobación del conjunto de datos completo podemos observar que hay otra variable discreta que también presenta valores perdidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar.isna().sum()[df_modelar.isna().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar[df_modelar.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en realidad estos 40 valores perdidos corresponden unicamente con 20 registros de nuestro conjunto de datos, ya que casualmente los valores perdidos se presentan de forma simultánea tanto en la variable `CONTRUCTIONYEAR` como `MAXBUILDINGFLOOR`. Dado los valores perdidos pertenecen a las clases más desbalanceadas de nuestro conjunto se debe de tomar la decisión de qué hacer con los valores perdidos. \n",
    "\n",
    "También se debe de tener en cuenta que la variable `CONTRUCTIONYEAR` tiene un orden que indica la calidad del terreno, por lo que no podríamos establecer cualquier valor. Esta decisión se resolverá más adelante.\n",
    "\n",
    "Como información adicional, se realiza una gráfica interactiva para visualizar el número de registros de cada clase en función del año de construcción de los edificios colindantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_year_class(Year=2017):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.countplot(df_modelar[df_modelar['CONTRUCTIONYEAR']==Year]['CONTRUCTIONYEAR'], hue=df_modelar['CLASE'])\n",
    "    plt.ylabel('Number')\n",
    "    plt.show()\n",
    "    print(f'-----------Year {Year}---------')\n",
    "    print(df_modelar[df_modelar['CONTRUCTIONYEAR']==Year]['CLASE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_year_class,\n",
    "         Year = np.sort(df_modelar['CONTRUCTIONYEAR'].unique()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_year = df_modelar.groupby('CONTRUCTIONYEAR')['CLASE'].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "df_group_year.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos centramos en aquellas clases que son menos predominantes, ya que como el problema está desbalanceado, la clase Residencial es predominante.\n",
    "En la siguiente gráfica observamos que entre los años 1925 y 2017 los edificios predominantes son aquellos que presentan la clase industrial y publica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_group_year.columns.tolist()[0:-2]\n",
    "classes.append('RETAIL')\n",
    "fig, ax = plt.subplots(figsize=(20,12))\n",
    "df_group_year[classes].plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la variable restante de este grupo, `Maxbuildingfloor` sabemos que hace referencia a la altura máxima de los registros colindantes al terreno en cuestión y no al propio. De todas formas realizamos un análisis de la misma para conocer un poco más sobre ella y obtener algunos datos que nos puedan otorgar información adicional al problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Diferentes valores de la variable: \\n\", df_modelar['MAXBUILDINGFLOOR'].unique())\n",
    "print(\"Total: \", len(df_modelar['MAXBUILDINGFLOOR'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que presenta un <b>total de 27 valores distintos</b> esta variable, en los cuales se incluyen también los valores perdidos ya que por el momento no han sido tratados y se hará más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.countplot(x='MAXBUILDINGFLOOR', data=df_modelar, hue='CLASE')\n",
    "plt.ylabel('Number')\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resulta imposible realizar un análisi de esta variable mediante este tipo de gráficos debido a la gran diferencia que existe entre sus valores y la variable objetivo, se ha optado por realizar un gráfico interactivo que nos permita entrar más en detalle en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_floor_class(Floor=1):\n",
    "    plt.figure(figsize=(20,12))\n",
    "    sns.countplot(df_modelar[df_modelar['MAXBUILDINGFLOOR']==Floor]['MAXBUILDINGFLOOR'], hue=df_modelar['CLASE'])\n",
    "    plt.ylabel('Number')\n",
    "    plt.show()\n",
    "    print(f'-----------Floor {Floor}---------')\n",
    "    print(df_modelar[df_modelar['MAXBUILDINGFLOOR']==Floor]['CLASE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_floor_class,\n",
    "         Floor = np.sort(df_modelar['MAXBUILDINGFLOOR'].unique())[:-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Información relativa al valor perdido con respecto a la variable MAXBUILDINGFLOOR\n",
    "sns.countplot(x='CLASE', data=df_modelar[df_modelar['MAXBUILDINGFLOOR'].isna()])\n",
    "plt.ylabel('Number');\n",
    "print(f'-----------Floor NaN---------')\n",
    "print(df_modelar[df_modelar['MAXBUILDINGFLOOR'].isna()]['CLASE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta gráfica podemos ver la clasificación que obtienen los registros en los cuales nos encontramos con un valor perdido en la variable `MAXBUILDINGFLOOR`. Como se obserca, 15 de ellos pertenecen a la clase minoritaria del conjunto de datos `AGRICULTURE` y el resto a `INDUSTRIAL` y `RETAIL`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section32\"></a>\n",
    "### <font color=\"#004D7F\">3.2 Tratamiento de las columnas discretas </font>\n",
    "\n",
    "\n",
    "En relación a estas columnas, dos aspectos muy relevantes de cara a la construcción de un modelo con `scikit-learn` son: el número de valores que puede tomar cada una; y si existe una relación de orden entre estos valores. Estos factores determinan el tipo de transformación que se ha de hacer. Existen cuatro posibilidades:\n",
    "\n",
    "* Cuando la columna toma dos valores, se puede binarizar y convertir a numérica diréctamente. \n",
    "* Si el tamaño del conjunto de valores es mayor que dos, y no existe una relación de orden entre ellos, se aplica `One Hot Encoding` (se aplicará posteriormente en el `Pipeline` de transformaciones).\n",
    "* Si existe una relación de orden, los valores se transforman a numéricos, sustituyendo cada valor por su orden. \n",
    "* Si el conjunto de valores extremadamente grande se ha de explorar, ya que es muy posible que se trate de un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recordar que la variable CLASE también es discreta pero se ha eliminado al ser la varible objetivo\n",
    "num_values_dis_df_col = [(col, len(df_modelar[col].value_counts())) for col in dis_df_columns]\n",
    "num_values_dis_df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta variable en concreto tenemos una información adicional y es que se trata de una variable categórica representativa de la calidad y que tiene un órden:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> <b>MAYOR a MENOR CALIDAD: A > B > C > 1 > 2 > 3 >...> 8 > 9</b>\n",
    "</div>\n",
    "\n",
    "Por lo tanto, lo primero que debemos de hacer es establecer ese órden en estas variable en el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cadastralquality(value):\n",
    "    dic = {'A': 11, 'B': 10, 'C': 9}\n",
    "    if value in dic:\n",
    "        return dic[value]\n",
    "    else:\n",
    "        try:\n",
    "            return 9 - int(value)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "        \n",
    "def process_cadastral(df):\n",
    "    df['CADASTRALQUALITYID'] = df['CADASTRALQUALITYID'].apply(process_cadastralquality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_preprocess_function.append(process_cadastral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_cadastral(df_modelar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelar['CADASTRALQUALITYID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(x=df_modelar['CADASTRALQUALITYID'], data=df_modelar, hue=\"CLASE\", dodge=False)\n",
    "plt.title('Distribución de muestras');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos indicadores de calidad se pueden agrupar junto a su clasificación en la variable `CLASE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVISAR\n",
    "# df_modelar[df_modelar['CLASE']=='AGRICULTURE']['MAXBUILDINGFLOOR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadatral_by_class = df_modelar.groupby('CADASTRALQUALITYID')['AREA'].mean()\n",
    "cadatral_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadatral_by_class = df_modelar.groupby('CLASE')['CADASTRALQUALITYID'].value_counts().unstack()\n",
    "cadatral_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadatral_by_class.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar tras la agrupación, la calidad mayoritaria es la 5, que representaría aproximadamente la calidad media en el catastro. Por otro lado, la calidad más alta (11) presenta un número muy bajo de registros en comparación con el resto y la mayor parte de los mismos están clasificados como `RESIDENTIAL`. Si nos fijamos, también podemos observar que los registros clasificados como `AGRICULTURE` son los que peor calidad catrastral presental.\n",
    "\n",
    "Además, los registros clasificados como `INDUSTRIAL` se situan entorno a una calidad de nivel 3 y 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## <font color=\"#004D7F\"> 4. Preprocesamiento del conjunto de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado el análisis exploratorio de los datos y de las variables del _dataframe_ proporcionado, se deben de llevar a cabo las acciones de preprocesamiento necesarias.\n",
    "\n",
    "Para ello haremos uso de la lista de funciones creada con anterioridad. En ella se han ido incluyendo las funciones necesarias y aunque en este caso unicamente contamos con una, como se ve a continuación, podemos añadir las que consideremos oportunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPasos de preprocesamiento: \")\n",
    "for step, function in enumerate(list_preprocess_function):\n",
    "    print(\"\\t {:d}: {:s}\".format(step, function.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda aplica cada una de las funciones de la lista al _dataframe_ que le introduzcamos como parámetro y devolverá el _dataframe_ modificado al realizar el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, list_preprocess_function):\n",
    "    for func in list_preprocess_function:\n",
    "        func(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(df_modelar, list_preprocess_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez aplicadas cada una de las funciones anteriores, es importarte eliminar o añadir a las dos listas que manejabamos de vairables categórigas y numéricas las columnas que realmente pertenecen a ese tipo. Por ello, como la variable `CADASTRALQUALITYID` se ha tratado al final como una numérica, debemos de incluirla y eliminarla de las listas correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_df_columns.remove('CADASTRALQUALITYID')\n",
    "number_df_columns.append('CADASTRALQUALITYID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, a nivel informativo, se muestra en la siguiente celda las distintas columnas que se incluyen en cada una de las listas en función del tipo de variable que representan tras realizar el preprocesamiento de correspondiente y sabiendo que se ha eliminado la variable objetivo CLASE de la lista de variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variables categóricas: ', dis_df_columns, end='\\n\\n')\n",
    "print('Variables numéricas: ', num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section41\"></a>\n",
    "### <font color=\"#004D7F\">4.1 Creación de un Pipeline para la transformación </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los _Pipelines_ son una herramienta extremadamente simple pero que nos permite ahorrar y gestionar flujos de trabajo en este tipo de problemas. Los _Pipelines_ están diseñados para aplicar una serie de transformaciones de datos seguidas por la aplicación de un estrimador.\n",
    "\n",
    "Surgen por la necesidad de tratar de manera separada los datos, es decir, las variables categóricas se deben de tratar de distinta forma a las variables numéricas pero, posteriormente se deben de unir (mediante el uso del objeto ColumnTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, en la siguiente celda se crea un objeto de tipo _Pipeline_ que permite definir el proceso de transformación de las variables numéricas. Este objeto consiste en lo siguiente:\n",
    "* Imputación de los valores perdidos mediante SimpleImputer, el cual tiene como estrategia _constant_ para completar con un valor establecido por nosotros mismos, que es el -1, que indicaría la peor calidad del catastro. \n",
    "* Normalización a media cero y desviación uno mediante el uso de StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformación que se le va a aplicar a las columnas numéricas\n",
    "num_transformer = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value=-1)),\n",
    "                             ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual formar que se ha realizado para las variables numéricas, también se debe de realizar para las variables categóricas. En este caso, el objeto _Pipeline_ creado consta de lo siguiente:\n",
    "* Imputación de los valores perdidos mediante SimpleImputer reemplazando los valores perdidos por una etiqueta, de forma similar al caso anterior.\n",
    "* Transformación de las variables categóricas a etiquetas binarias mediante un objeto OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i> Importante. Esto es solo una propuesta de trabajo y de desarrollo. En nuestro caso no contamos con variables categóricas a tratar, por lo que no es necesario la realización de este _Pipeline_ pero se adjunta a nivel informativo o por si en algun caso se añade alguna variable de este tipo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformación que se le aplicaría a las columnas categóricas, en el caso de que existan\n",
    "cat_transformer = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                           ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po último, una vez se tienen las dos secuencias de transformaciones anteriores, se deben de unir y aplicar a las características correspondientes. Para ello se hace uso de un objeto incluido recientemente en la librería de _scikit-learn_ llamado `ColumnTransformer`, que permite tratar por separado estas columnas.\n",
    "\n",
    "En nuestro caso, este objeto se ha creado unicamente con el _Pipeline_ de las variables numéricas porque no tenemos variables categóricas a tratar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformador que se aplica a cada una de las columnas en función a lo declarado previamente\n",
    "df_modelar_trans = ColumnTransformer(transformers=[('num', num_transformer, number_df_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "## <font color=\"#004D7F\"> 5. Construcción de modelos </font>\n",
    "\n",
    "El principal problema que tenemos a la hora de construir los modelos es el del gran desbalanceo de las clases que hemos visto en el apartado [3. Análisis exploratorio de los datos](#section3), donde tenemos la clase `RESIDENTIAL` como clase predominante con respecto al resto.\n",
    "\n",
    "Para solventar este problema, vamos a utilizar la estrategia de **apilamiento de modelos**, es decir, utilizar un primer modelo que nos discrimine entre las clases `RESIDENTIAL` y `NO RESIDENTIAL`, la cual estará constituida por todos los registros cuya clase sea distinta a residencial. Lo que conseguimos con esto es hacer filtro de los datos y una vez obtengamos los registros cuya predicción sea `NO RESIDENTIAL` entonces los pasaremos a un segundo modelo, el cual estará entrenado con todas las clases, pero más balanceadas, manteniendo las distribuciones originales.\n",
    "\n",
    "Dado que el primer modelo va seguir estando desbalanceado, puesto que disponemos de 90173 registros cuya clase es residencial y 23230 cuya clase es distinta a residencial, vamos a utilizar una técnica de **Oversampling** con la que vamos a generar registros sintéticos similares a los que tenemos en el conjunto de datos inicial mediante el algoritmo `SMOTE` (*Synthetic Minority Over-sampling Technique*) cuyo funcionamiento queda reflejado en el siguiente artículo: https://arxiv.org/pdf/1106.1813.pdf.\n",
    "\n",
    "Con ello vamos a generar un modelo binario lo más balanceado posible, igualando los registros residenciales y los no residenciales, como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,6))\n",
    "sns.barplot(x=['RESIDENTIAL', 'NO RESIDENTIAL'], y=[90173, 23230], ax=ax1)\n",
    "sns.barplot(x=['RESIDENTIAL', 'NO RESIDENTIAL'], y=[90173, 90173], ax=ax2)\n",
    "ax1.set_title('Antes de aplicar SMOTE')\n",
    "ax2.set_title('Después de aplicar SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el nuevo conjunto de datos balanceado, entrenaremos una serie de modelos realizando una búsqueda exhaustiva de hiperparametros para conseguir obtener el mejor modelo con respecto a unas métricas establecidas.\n",
    "\n",
    "Una vez obtengamos nuestro modelo binario, será hora de generar el segundo modelo, el modelo multietiqueta. Este modelo contemplará todas las clases presentes en el problema, con la condición de que ahora la clase mayoritaria no será la residencial, puesto que ha sido previamente clasificada por el modelo binario. Como consecuencia de ello, este segundo modelo será necesario entrenarlo con una distribución acorde al nuevo escenario de datos, el cual se puede observar a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i>\n",
    "Los ejemplos aquí mostrados son representativos, pero los datos **no** son los reales debido a que no se ha realizado todavía el proceso.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_filtrados = df_modelar[df_modelar['CLASE']!='RESIDENTIAL']['CLASE'].value_counts() \\\n",
    "        .append(pd.Series(index=['RESIDENTIAL'], data=[678]))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,6))\n",
    "sns.countplot(df_modelar['CLASE'], order=list_class_order, ax=ax1)\n",
    "sns.barplot(x=datos_filtrados.index, y=datos_filtrados.values, order=list_class_order, ax=ax2)\n",
    "ax1.set_title('Antes de aplicar el modelo binario')\n",
    "ax2.set_title('Después de aplicar el modelo binario')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, tras aplicar el modelo binario, hemos conseguido balancear manteniendo las distribuciones originales del resto de clases que no son la residencial. Puede resultar curioso, el porqué hay registros de la clase `RESIDENTIAL` una vez aplicado el modelo binario, y esto es debido a que el modelo posiblemente presente fallos a la hora de clasificar, por lo que habrá casos residenciales que los clasifique como no residenciales. Con esta propuesta, la clase residencial también podrá ser clasificada, tendrá por así decirlo una \"*segunda oportunidad*\", ya que lo normal es que el modelo binario previamente generado presente un % de error en su predicción y con esta estrategia, corregimos en la medida de lo posible ese error producido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section51\"></a>\n",
    "### <font color=\"#004D7F\">5.1 Construcción de modelo binario </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos hacer es generar una nueva variable en función a la variable `CLASE` que nos permita identificar si el registro hace referencia a la clase `RESIDENTIAL` o `NO RESIDENTIAL`. Para ello, vamos a hacer uso de la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assing_subclass(df):\n",
    "    df['SUBCLASE'] = (df['CLASE'] == 'RESIDENTIAL').astype(int)\n",
    "\n",
    "assing_subclass(df_modelar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez generada la nueva variable, es hora de utilizar el algoritmo **SMOTE**. El cual nos va a permitir realizar un aumento de registros de manera sintética con respecto aquellos de la clase minoritaria, es decir, la no residencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos en conjunto de datos, de la variable a predecir, que en este caso es la nueva que hemos generado en el paso previo\n",
    "X = df.drop(columns=df.columns[-2:]).copy() #Importante, debemos eliminar también la variable clase para no afectar en el entrenamiento\n",
    "y = df['SUBCLASE'].copy() #La clase a predecir va a ser la que hemos generado y la que nos va a permitir discretizar\n",
    "\n",
    "#Realizamos un SMOTE para la clase minoritaria para así balancear el conjunto de datos\n",
    "sm = SMOTE(random_state=10, sampling_strategy='not majority')\n",
    "X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, el nuevo conjunto de datos generado debemos dividirlo en un conjunto de train y de test para poder entrenar nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con respecto a que modelos de clasificación debemos entrenar, hemos optado por los siguientes algoritmos de clasificación. A pesar de que hay una gran variedad, estos son los que nosotros conocemos con mayor precisión y nos sentimos más cómodos de utilizar:\n",
    "- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">LogisticRegression</a>\n",
    "- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">DecisionTreeClassifier</a>\n",
    "- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">RandomForestClassifier</a>\n",
    "- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">SVM</a>\n",
    "- <a href=\"https://xgboost.readthedocs.io/en/latest/python/python_api.html\">XGBOOST</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = xgb.XGBClassifier(random_state=10)\n",
    "# \n",
    "# parameters = {\n",
    "#         'clas__min_child_weight': [1, 5, 10],\n",
    "#         'clas__gamma': [0.5, 1, 1.5, 2, 5],\n",
    "#         'clas__subsample': [0.6, 0.8, 1.0],\n",
    "#         'clas__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#         'clas__n_estimators': [100, 200, 500],\n",
    "#         'clas__max_depth': [2, 5, 10, 20, 30]\n",
    "# }\n",
    "# \n",
    "# pipe_xgboost = Pipeline(steps=[('prep', df_minsait_trans), ('clas', xgb_model)])\n",
    "# \n",
    "# GS = GridSearchCV(pipe_xgboost, parameters, cv=5, n_jobs=-1, scoring='accuracy', refit=True, verbose=1)\n",
    "# GS.fit(X_train, y_train)\n",
    "#     \n",
    "# print(\"Mejor score: \", GS.best_score_)\n",
    "# print(\"Mejore configuración de parámetros: \", GS.best_params_)\n",
    "# \n",
    "# pipe_xgboost = GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(random_state=10)\n",
    "\n",
    "#Parametros obtenidos del anterior gridsearch\n",
    "parameters = {\n",
    "        'clas__n_estimators': [100],\n",
    "        'clas__max_depth': [20]\n",
    "}\n",
    "\n",
    "pipe_xgboost = Pipeline(steps=[('prep', df_minsait_trans), ('clas', xgb_model)])\n",
    "\n",
    "GS = GridSearchCV(pipe_xgboost, parameters, cv=5, n_jobs=-1, scoring='accuracy', refit=True, verbose=1)\n",
    "GS.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Mejor score: \", GS.best_score_)\n",
    "print(\"Mejore configuración de parámetros: \", GS.best_params_)\n",
    "\n",
    "pipe_xgboost = GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_xgboost.predict(X_test)\n",
    "show_results(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(pipe_xgboost, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la importancia de las variables y mostrar el rankinsg  **EXPLICAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae las importancias\n",
    "importances = pipe_xgboost['clas'].feature_importances_\n",
    "# Extrae los índices ordenados de menor a mayor\n",
    "ranking = np.argsort(importances)\n",
    "\n",
    "#Ahora que hemos obtenido las importancias de las variables y su ranking, vamos a dibujarlo\n",
    "plt.figure(figsize=(28,20))\n",
    "plt.title(\"Ranking de importancias de las variables con XGBOOST\")\n",
    "plt.barh(range(df.shape[1]-2), importances[ranking],color=sns.diverging_palette(220, 20, n=55),align='center')\n",
    "plt.yticks(range(df.shape[1]-2), X.columns[ranking], fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipe_xgboost, open('./models/binary_smote_15_03_xgboost_10_100.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section52\"></a>\n",
    "### <font color=\"#004D7F\">5.2 Construcción de modelo multietiqueta </font>\n",
    "\n",
    "EXPLICACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
